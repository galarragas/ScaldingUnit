{"name":"ScaldingUnit","tagline":"TDD utils for Scalding developers ","body":"# ScaldingUnit - TDD utils for Scalding developers\r\n\r\n![Scalding logo BW](https://github.com/scalding-io/ScaldingUnit/raw/master/logo/scaldingBW.png \"Scalding\")\r\n\r\n## NOTE: ScaldingUnit merged into Scalding\r\n\r\nScaldingUnit has been merged with some small changes (mostly the name of the class to extend has been renamed from TestInfrastructure to BddDsl)\r\ninto scalding-core development branch in date 30th Jan 2014.\r\nCurrent project will be kept active to be used for version of scalding prior to 0.9.\r\nScaldingUnit is also not working with Scalding 0.9.1 so please switch to using the BddDsl trait instead of the TestInfrastructure for project using Scalding version > 0.9\r\n\r\n## Aim\r\n\r\nThe aim of this project is to allow user to write Scalding (https://github.com/twitter/scalding) map-reduce jobs in a more modular and test-driven way.\r\nIt is based on the experience done in the Big Data unity at BSkyB where it originated and is currently used and maintained.\r\nIt essentially provides a test harness to support the decomposition of a Scalding Map-Reduce Job into a series of smaller steps,\r\neach one testable independently before being assembled into the main Job that will then be tested as a whole using Scalding-based\r\ntests.\r\n\r\n[![Build Status](https://api.travis-ci.org/galarragas/ScaldingUnit.png)](https://api.travis-ci.org/scalding-io/ScaldingUnit.png)\r\n\r\n## What does it look like\r\n\r\nA test written with scalding unit look as shown below:\r\n\r\nWith ScalaTest\r\n\r\n```scala\r\nclass SampleJobPipeTransformationsSpec extends FlatSpec with ShouldMatchers with TupleConversions with TestInfrastructure {\r\n  \"A sample job pipe transformation\" should \"add user info\" in {\r\n    Given {\r\n      List((\"2013/02/11\", 1000002l, 1l)) withSchema EVENT_COUNT_SCHEMA\r\n    } And {\r\n      List( (1000002l, \"stefano@email.com\", \"10 Downing St. London\") ) withSchema USER_DATA_SCHEMA\r\n    } When {\r\n      (eventCount: RichPipe, userData: RichPipe) => eventCount.addUserInfo(userData)\r\n    } Then {\r\n      buffer: mutable.Buffer[(String, Long, String, String, Long)] =>\r\n        buffer.toList shouldEqual List( (\"2013/02/11\", 1000002l, \"stefano@email.com\", \"10 Downing St. London\", 1l) )\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nor with Specs2\r\n\r\n```scala\r\nimport org.specs2.{mutable => mutableSpec}\r\n\r\nclass SampleJobPipeTransformationsSpec2Spec extends mutableSpec.SpecificationWithJUnit with TupleConversions with TestInfrastructure {\r\n\r\n  // See: https://github.com/twitter/scalding/wiki/Frequently-asked-questions\r\n\r\n  \"A sample job pipe transformation\" should {\r\n     Given {\r\n      List((\"2013/02/11\", 1000002l, 1l)) withSchema EVENT_COUNT_SCHEMA\r\n    } And {\r\n      List((1000002l, \"stefano@email.com\", \"10 Downing St. London\")) withSchema USER_DATA_SCHEMA\r\n    } When {\r\n      (eventCount: RichPipe, userData: RichPipe) => eventCount.addUserInfo(userData)\r\n    } Then {\r\n      buffer: mutable.Buffer[(String, Long, String, String, Long)] =>\r\n        \"add user info\" in {\r\n          buffer.toList shouldEqual List((\"2013/02/11\", 1000002l, \"stefano@email.com\", \"10 Downing St. London\", 1l))\r\n        }\r\n    }\r\n\r\n  }\r\n}\r\n```\r\n\r\n\r\nWhere `addUserInfo` is a function joining two richPipes to generate an enriched one.\r\n\r\n## Usage\r\n\r\nTo add the dependency on maven include:\r\n\r\n```xml\r\n<dependency>\r\n  <groupId>com.pragmasoft</groupId>\r\n  <artifactId>scalding-unit</artifactId>\r\n  <version>0.6</version>\r\n</dependency>\r\n```\r\n\r\nand repository:\r\n\r\n```xml\r\n<repository>\r\n  <id>conjars.org</id>\r\n  <url>http://conjars.org/repo</url>\r\n</repository>\r\n```\r\n\r\nThe code is not currently working correctly if you are using Scalding 0.9.rc4.\r\nThe code supporting the 0.9 version is currently in a branch until 0.9 will be GA. To use it include the following dependency:\r\n\r\n```xml\r\n<dependency>\r\n  <groupId>com.pragmasoft</groupId>\r\n  <artifactId>scalding-unit</artifactId>\r\n  <version>0.5-scalding0.9</version>\r\n</dependency>\r\n```\r\n\r\nSupport for 0.9.rc4 hasn't been updated to version 0.6 yet.\r\n\r\n## Using ScaldingUnit with Different Versions of Scala\r\n\r\nScaldingUnit is built with Scala 2.10 but has been tested with Scala 2.10 and from version 0.5 with 2.9.2.\r\nRefer to the example directories (examples and examples2.9) to see how to use it with ScalaTest and Specs2.\r\n\r\nThe explanation below refers to Scala 2.10 code but the changes to apply to use 2.9 are minimal.\r\n\r\n## Motivation and details\r\n\r\n## Writing and testing Scalding Jobs without ScaldingUnit\r\n\r\nA Scalding job consists in a series of transformations applied to one or more sources in order to create one or more\r\noutput resources or sinks. A very simple example taken from the Scalding documentations is as follows.\r\n\r\n```scala\r\npackage com.twitter.scalding.examples\r\n\r\nimport com.twitter.scalding._\r\n\r\nclass WordCountJob(args : Args) extends Job(args) {\r\n    TextLine( args(\"input\") )\r\n     .flatMap('line -> 'word) { line : String => tokenize(line) }\r\n     .groupBy('word) { _.size }\r\n     .write( Tsv( args(\"output\") ) )\r\n\r\n    // Split a piece of text into individual words.\r\n    def tokenize(text : String) : Array[String] = {\r\n     // Lowercase each word and remove punctuation.\r\n     text.toLowerCase.replaceAll(\"[^a-zA-Z0-9\\\\s]\", \"\").split(\"\\\\s+\")\r\n    }\r\n}\r\n```\r\n\r\nThe transformations are defined as operations on a `cascading.pipe.Pipe` class or on the richer wrapper `com.twitter.scalding.RichPipe`.\r\nScalding provides a way of testing Jobs via the com.twitter.scalding.JobTest class. This class allows to specify values for the different\r\nJob sources and to specify assertions on the different job sinks.\r\nThis approach works very well to do end to end test on the Job and is good enough for small jobs as the one described above\r\nbut doesn't encourage modularisation when writing more complex Jobs. That's the reason we started working on ScaldingUnit.\r\n\r\n## Checking your Pipes, modular unit test of your Scalding Job\r\n\r\nWhen the Job logic become more complex it is very helpful to decompose its work in simpler functions to be tested independently before being\r\naggregated into the Job.\r\n\r\nLet's consider the following Scalding Job (it is still a simple one for reason of space but should give the idea):\r\n\r\n```scala\r\nclass SampleJob(args: Args) extends Job(args) {\r\n  val INPUT_SCHEMA = List('date, 'userid, 'url)\r\n  val WITH_DAY_SCHEMA = List('date, 'userid, 'url, 'day)\r\n  val EVENT_COUNT_SCHEMA = List('day, 'userid, 'event_count)\r\n  val OUTPUT_SCHEMA = List('day, 'userid, 'email, 'address, 'event_count)\r\n\r\n  val USER_DATA_SCHEMA = List('userid, 'email, 'address)\r\n\r\n  val INPUT_DATE_PATTERN: String = \"dd/MM/yyyy HH:mm:ss\"\r\n\r\n  Osv(args(\"eventsPath\")).read\r\n    .map('date -> 'day) {\r\n           date: String => DateTimeFormat.forPattern(INPUT_DATE_PATTERN).parseDateTime(date).toString(\"yyyy/MM/dd\");\r\n         }\r\n    .groupBy(('day, 'userid)) { _.size('event_count) }\r\n    .joinWithLarger('userid -> 'userid, Osv(args(\"userInfoPath\")).read).project(OUTPUT_SCHEMA)\r\n    .write(Tsv(args(\"outputPath\")))\r\n}\r\n```\r\n\r\nIt is possible to identify three main operation performed during this task. Each one is providing a identifiable and\r\n autonomous transformation to the pipe, just relying on a specific input schema and generating a transformed pipe with a\r\n potentially different output schema. Following this idea it is possible to write the Job in this way:\r\n\r\n```scala\r\nimport SampleJobPipeTransformations._\r\n\r\nclass SampleJob(args: Args) extends Job(args) {\r\n\r\n  Osv(args(\"eventsPath\")).read\r\n    .addDayColumn\r\n    .countUserEventsPerDay\r\n    .addUserInfo(Osv(args(\"userInfoPath\")).read)\r\n    .write(Tsv(args(\"outputPath\")))\r\n}\r\n```\r\n\r\nWhere the single operations have been extracted as basic functions into a separate class that is not a Scalding Job:\r\n\r\n```scala\r\npackage object SampleJobPipeTransformations  {\r\n\r\n  val INPUT_SCHEMA = List('date, 'userid, 'url)\r\n  val WITH_DAY_SCHEMA = List('date, 'userid, 'url, 'day)\r\n  val EVENT_COUNT_SCHEMA = List('day, 'userid, 'event_count)\r\n  val OUTPUT_SCHEMA = List('day, 'userid, 'email, 'address, 'event_count)\r\n\r\n  val USER_DATA_SCHEMA = List('userid, 'email, 'address)\r\n\r\n  val INPUT_DATE_PATTERN: String = \"dd/MM/yyyy HH:mm:ss\"\r\n\r\n  implicit def wrapPipe(pipe: Pipe): SampleJobPipeTransformationsWrapper = new SampleJobPipeTransformationsWrapper(new RichPipe(pipe))\r\n  implicit class SampleJobPipeTransformationsWrapper(val self: RichPipe) extends PipeOperations {\r\n\r\n    /**\r\n     * Input schema: INPUT_SCHEMA\r\n     * Output schema: WITH_DAY_SCHEMA\r\n     * @return\r\n     */\r\n    def addDayColumn = self.map('date -> 'day) {\r\n      date: String => DateTimeFormat.forPattern(INPUT_DATE_PATTERN).parseDateTime(date).toString(\"yyyy/MM/dd\");\r\n    }\r\n\r\n    /**\r\n     * Input schema: WITH_DAY_SCHEMA\r\n     * Output schema: EVENT_COUNT_SCHEMA\r\n     * @return\r\n     */\r\n    def countUserEventsPerDay = self.groupBy(('day, 'userid)) { _.size('event_count) }\r\n\r\n    /**\r\n     * Joins with userData to add email and address\r\n     *\r\n     * Input schema: WITH_DAY_SCHEMA\r\n     * User data schema: USER_DATA_SCHEMA\r\n     * Output schema: OUTPUT_SCHEMA\r\n     */\r\n    def addUserInfo(userData: Pipe) = self.joinWithLarger('userid -> 'userid, userData).project(OUTPUT_SCHEMA)\r\n  }\r\n}\r\n```\r\n\r\nThe main Job class is responsible of essentially dealing with the configuration, opening the input and output richPipes and\r\ncombining those macro operations. Using implicit transformations and value classes as shown above is possible to use those\r\noperations as if they were part of the RichPipe class.\r\n\r\nIt is now possible to test all this method independently and without caring about the source and sinks of the job and of\r\nthe way the configuration is given.\r\n\r\nThe specification of the transformation class is shown below:\r\n\r\n```scala\r\nclass SampleJobPipeTransformationsSpec extends FlatSpec with ShouldMatchers with TupleConversions with TestInfrastructure {\r\n  \"A sample job pipe transformation\" should \"add column with day of event\" in {\r\n    Given {\r\n      List( (\"12/02/2013 10:22:11\", 1000002l, \"http://www.youtube.com\") ) withSchema INPUT_SCHEMA\r\n    } When {\r\n      pipe: RichPipe => pipe.addDayColumn\r\n    } Then {\r\n      buffer: mutable.Buffer[(String, Long, String, String)] =>\r\n        buffer.toList(0) shouldEqual ((\"12/02/2013 10:22:11\", 1000002l, \"http://www.youtube.com\", \"2013/02/12\"))\r\n    }\r\n  }\r\n\r\n  it should \"count user events per day\" in {\r\n    def sortedByDateAndIdAsc( left: (String, Long, Long), right: (String, Long, Long)): Boolean =\r\n      (left._1 < right._1) || ((left._1 == right._1) && (left._2 < left._2))\r\n\r\n    Given {\r\n      List(\r\n          (\"12/02/2013 10:22:11\", 1000002l, \"http://www.youtube.com\", \"2013/02/12\"),\r\n          (\"12/02/2013 10:22:11\", 1000002l, \"http://www.youtube.com\", \"2013/02/12\"),\r\n          (\"11/02/2013 10:22:11\", 1000002l, \"http://www.youtube.com\", \"2013/02/11\"),\r\n          (\"15/02/2013 10:22:11\", 1000002l, \"http://www.youtube.com\", \"2013/02/15\"),\r\n          (\"15/02/2013 10:22:11\", 1000001l, \"http://www.youtube.com\", \"2013/02/15\"),\r\n          (\"15/02/2013 10:22:11\", 1000003l, \"http://www.youtube.com\", \"2013/02/15\"),\r\n          (\"15/02/2013 10:22:11\", 1000001l, \"http://www.youtube.com\", \"2013/02/15\"),\r\n          (\"15/02/2013 10:22:11\", 1000002l, \"http://www.youtube.com\", \"2013/02/15\")\r\n        ) withSchema WITH_DAY_SCHEMA\r\n    } When {\r\n      pipe: RichPipe => pipe.countUserEventsPerDay\r\n    } Then {\r\n      buffer: mutable.Buffer[(String, Long, Long)] =>\r\n        buffer.toList.sortWith(sortedByDateAndIdAsc(_, _)) shouldEqual List(\r\n                (\"2013/02/11\", 1000002l, 1l),\r\n                (\"2013/02/12\", 1000002l, 2l),\r\n                (\"2013/02/15\", 1000001l, 2l),\r\n                (\"2013/02/15\", 1000002l, 2l),\r\n                (\"2013/02/15\", 1000003l, 1l)\r\n              )\r\n    }\r\n  }\r\n\r\n\r\n  it should \"add user info\" in {\r\n    Given {\r\n      List((\"2013/02/11\", 1000002l, 1l)) withSchema EVENT_COUNT_SCHEMA\r\n    } And {\r\n      List( (1000002l, \"stefano@email.com\", \"10 Downing St. London\") ) withSchema USER_DATA_SCHEMA\r\n    } When {\r\n      (eventCount: RichPipe, userData: RichPipe) => eventCount.addUserInfo(userData)\r\n    } Then {\r\n      buffer: mutable.Buffer[(String, Long, String, String, Long)] =>\r\n        buffer.toList shouldEqual List( (\"2013/02/11\", 1000002l, \"stefano@email.com\", \"10 Downing St. London\", 1l) )\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThe TestInfrastructure trait is providing a BDD-like syntax to specify the Input to supply to the operation to test and\r\nto write the expectations on the results (the upper case syntax is caused by the fact that\r\nthe `then` keyword since it is deprecated from Scala 2.10).\r\n\r\nOnce the different steps have been tested thoroughly it is possible to combine them in the main Job and test the end to end\r\nbehavior using the JobTest class provided by Scalding.\r\n\r\n## Testing in Local and Hadoop Mode\r\n\r\nUntil version 0.5 all tests were executed in Local mode. From version 0.6 is possible to decide if testing in local or\r\nin Hadoop mode (the runHadoop method in JobTest).\r\n\r\nCheck the sample code for details but, in summary, mixing-in with the `TestInfrastructure` trait will cause the test to be\r\nexecuted locally as before. Using the trait `HadoopTestInfrastructure` will run the test on Hadoop.\r\nIf you want to execute selected test on Hadoop or to decide checking some configuration or execution parameter you can\r\nmix with the `MultiTestModeTestInfrastructure` and define the test mode in the test method or in the test class as follows:\r\n\r\n```scala\r\nclass SampleJobPipeConfigurableRunModeTransformationsSpec2Spec extends mutableSpec.SpecificationWithJUnit with TupleConversions with MultiTestModeTestInfrastructure {\r\n\r\n  // See: https://github.com/twitter/scalding/wiki/Frequently-asked-questions\r\n\r\n  import TestRunMode._\r\n\r\n  \"You can run on hadoop\" should {\r\n\r\n    implicit val _ = testOnHadoop\r\n\r\n    Given {\r\n      List((\"12/02/2013 10:22:11\", 1000002l, \"http://www.youtube.com\")) withSchema INPUT_SCHEMA\r\n    } When {\r\n      pipe: RichPipe => pipe.addDayColumn\r\n    } Then {\r\n      buffer: mutable.Buffer[(String, Long, String, String)] =>\r\n        \"add column with day of event\" in {\r\n          buffer.toList(0) shouldEqual ((\"12/02/2013 10:22:11\", 1000002l, \"http://www.youtube.com\", \"2013/02/12\"))\r\n        }\r\n    }\r\n  }\r\n\r\n  \"You can run locally\" should {\r\n\r\n    implicit val _ = testLocally\r\n\r\n    Given {\r\n      List((\"12/02/2013 10:22:11\", 1000002l, \"http://www.youtube.com\")) withSchema INPUT_SCHEMA\r\n    } When {\r\n      pipe: RichPipe => pipe.addDayColumn\r\n    } Then {\r\n      buffer: mutable.Buffer[(String, Long, String, String)] =>\r\n        \"add column with day of event\" in {\r\n          buffer.toList(0) shouldEqual ((\"12/02/2013 10:22:11\", 1000002l, \"http://www.youtube.com\", \"2013/02/12\"))\r\n        }\r\n    }\r\n  }\r\n\r\n  \"You can run decide using some configuration parameter or randomly\" should {\r\n\r\n    implicit val _ = if( (System.currentTimeMillis % 2) == 0) testLocally else testOnHadoop\r\n\r\n    Given {\r\n      List((\"12/02/2013 10:22:11\", 1000002l, \"http://www.youtube.com\")) withSchema INPUT_SCHEMA\r\n    } When {\r\n      pipe: RichPipe => pipe.addDayColumn\r\n    } Then {\r\n      buffer: mutable.Buffer[(String, Long, String, String)] =>\r\n        \"add column with day of event\" in {\r\n          buffer.toList(0) shouldEqual ((\"12/02/2013 10:22:11\", 1000002l, \"http://www.youtube.com\", \"2013/02/12\"))\r\n        }\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\n## Content\r\n\r\nThe repository contains two projects:\r\n\r\n - scalding-unit: the main project, providing the test framework\r\n - examples: a set of examples to describe the design approach described here\r\n\r\n## License\r\n\r\nCopyright 2013 PragmaSoft Ltd.\r\n\r\nLicensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}